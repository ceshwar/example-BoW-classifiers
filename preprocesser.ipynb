{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really love that shirt at\n",
      "Titanic tragedy could have been prevented Economic Times Telegraph co ukTitanic tragedy could have been preve\n",
      "I am at Starbucks 7419 3rd ave at 75th Brooklyn\n"
     ]
    }
   ],
   "source": [
    "####http://stackoverflow.com/questions/8376691/how-to-remove-hashtag-user-link-of-a-tweet-using-regular-expression\n",
    "import re,string\n",
    "\n",
    "def strip_links(text):\n",
    "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links         = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], ', ')    \n",
    "    return text\n",
    "\n",
    "def strip_all_entities(text):\n",
    "    ###strip only @mentions and not #hashtags!\n",
    "    entity_prefixes = ['@']\n",
    "#     entity_prefixes = ['@','#']\n",
    "    for separator in  string.punctuation:\n",
    "        if separator not in entity_prefixes :\n",
    "            text = text.replace(separator,' ')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "tests = [\n",
    "    \"@peter I really love that shirt at #Macy. http://bet.ly//WjdiW4\",\n",
    "    \"@shawn Titanic tragedy could have been prevented Economic Times: Telegraph.co.ukTitanic tragedy could have been preve... http://bet.ly/tuN2wx\",\n",
    "    \"I am at Starbucks http://4sh.com/samqUI (7419 3rd ave, at 75th, Brooklyn)\",\n",
    "]\n",
    "for t in tests:\n",
    "    print strip_all_entities(strip_links(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_examples_happy.txt\n",
      "Done:  17740\n",
      "neg_examples_sad.txt\n",
      "Done:  17740\n",
      "pos_examples_PosSentiment.txt\n",
      "Done:  100000\n",
      "neg_examples_NegSentiment.txt\n",
      "Done:  100000\n"
     ]
    }
   ],
   "source": [
    "filename = ['pos_examples_happy.txt', 'neg_examples_sad.txt','pos_examples_PosSentiment.txt', 'neg_examples_NegSentiment.txt']\n",
    "# filename = ['pos_examples_happy.txt']\n",
    "for file_ in filename:\n",
    "    print file_\n",
    "    target = open('data/nohashtags'+file_, 'w')\n",
    "    count = 0\n",
    "\n",
    "    with open('data/'+file_, 'r') as f:\n",
    "        pos_tweets = f.readlines()\n",
    "        for tweet in pos_tweets:\n",
    "            count += 1\n",
    "            target.write(strip_all_entities(strip_links(tweet)))\n",
    "            target.write('\\n')\n",
    "\n",
    "    print \"Done: \", count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data = COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "###test on preprocessed data\n",
    "\n",
    "##### load training data\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# suffix = \"\"\n",
    "suffix = \"preprocessed_\"\n",
    "###read tagged sentiment Tweets\n",
    "with open('data/'+suffix+'pos_examples_PosSentiment.txt', 'r') as f:\n",
    "    pos_tweets = f.readlines()\n",
    "\n",
    "with open('data/'+suffix+'neg_examples_NegSentiment.txt', 'r') as f:\n",
    "    neg_tweets = f.readlines()\n",
    "    \n",
    "class_label = 0\n",
    "train_pos = pd.DataFrame(pos_tweets, columns = ['text'])\n",
    "X0 = train_pos.text\n",
    "y0 = np.full(shape=len(X0), fill_value=class_label)\n",
    "\n",
    "###create random 80-20 split\n",
    "rows = random.sample(X0.index, int(0.8*len(X0)))\n",
    "X0_80 = X0.ix[rows]\n",
    "X0_20 = X0.drop(rows)\n",
    "y0_80 = y0[:int(0.8*len(X0))]\n",
    "y0_20 = y0[int(0.8*len(X0)):]\n",
    "###\n",
    "\n",
    "class_label = 1\n",
    "train_neg = pd.DataFrame(neg_tweets, columns = ['text'])\n",
    "X1 = train_neg.text\n",
    "y1 = np.full(shape=len(X1), fill_value=class_label)\n",
    "\n",
    "###create random 80-20 split\n",
    "rows = random.sample(X1.index, int(0.8*len(X1)))\n",
    "X1_80 = X1.ix[rows]\n",
    "X1_20 = X1.drop(rows)\n",
    "y1_80 = y1[:int(0.8*len(X1))]\n",
    "y1_20 = y1[int(0.8*len(X1)):]\n",
    "###\n",
    "\n",
    "X_train = pd.concat((X0_80,X1_80))\n",
    "y_train = np.concatenate((y0_80, y1_80))\n",
    "\n",
    "X_holdout = pd.concat((X0_20,X1_20))\n",
    "y_holdout = np.concatenate((y0_20, y1_20))\n",
    "\n",
    "### load testing data\n",
    "\n",
    "###read tagged sentiment Tweets\n",
    "with open('data/'+suffix+'pos_examples_happy.txt', 'r') as f:\n",
    "    pos_tweets = f.readlines()\n",
    "\n",
    "with open('data/'+suffix+'neg_examples_sad.txt', 'r') as f:\n",
    "    neg_tweets = f.readlines()\n",
    "    \n",
    "import numpy as np\n",
    "class_label = 0\n",
    "train_pos = pd.DataFrame(pos_tweets, columns = ['text'])\n",
    "X0 = train_pos.text\n",
    "y0 = np.full(shape=len(X0), fill_value=class_label)\n",
    "\n",
    "class_label = 1\n",
    "train_neg = pd.DataFrame(neg_tweets, columns = ['text'])\n",
    "X1 = train_neg.text\n",
    "y1 = np.full(shape=len(X1), fill_value=class_label)\n",
    "\n",
    "X_test = pd.concat((X0,X1))\n",
    "y_test = np.concatenate((y0, y1))\n",
    "\n",
    "###\n",
    "print \"Loading data = COMPLETE!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done getting top 5000 ngrams!\n"
     ]
    }
   ],
   "source": [
    "##Generate top 5000 ngrams and select random 50\n",
    "###print top 5000 features \n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "##vectorizer arguments blah!\n",
    "\n",
    "tokenizer=None#word_tokenize\n",
    "stop_words=nltk.corpus.stopwords.words(\"english\")#None\n",
    "stop_words.append('rt')\n",
    "ngram_range=(1, 3)\n",
    "lowercase=True\n",
    "max_features=5000\n",
    "binary=False\n",
    "dtype=np.float64\n",
    "\n",
    "###create vectorizer\n",
    "vectorizer = CountVectorizer(decode_error=\"ignore\",\n",
    "#                                tokenizer=tokenizer,\n",
    "                               stop_words=stop_words,\n",
    "                               ngram_range=ngram_range,\n",
    "                               lowercase=lowercase,\n",
    "                               binary=binary,\n",
    "                               dtype=dtype,\n",
    "                               max_features=max_features)\n",
    "\n",
    "train_fit = vectorizer.fit_transform(X_train)\n",
    "\n",
    "vocab = vectorizer.vocabulary_\n",
    "arr = (train_fit.toarray().sum(axis=0))\n",
    "\n",
    "with open(\"top5000.txt\", \"w\") as f:\n",
    "    for key in vocab:\n",
    "        try:\n",
    "            f.write(str(key) + ',' + str(arr[vocab[key]]) + \"\\n\")\n",
    "        except:\n",
    "            continue\n",
    "print \"Done getting top 5000 ngrams!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB 0.735725 0.732390200621 0.7429 0.737607665004\n",
      "SVC 0.742825 0.720259422196 0.79405 0.755356845585\n",
      "RF 0.73985 0.729785399502 0.76175 0.745425188375\n",
      "Prediction on Holdout (20%) = DONE!\n"
     ]
    }
   ],
   "source": [
    "### build pipeline; fit train; predict on holdout\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "##vectorizer arguments blah!\n",
    "\n",
    "tokenizer=None#word_tokenize\n",
    "stop_words=nltk.corpus.stopwords.words(\"english\")#None\n",
    "ngram_range=(1, 3)\n",
    "lowercase=True\n",
    "max_features=5000\n",
    "binary=False\n",
    "dtype=np.float64\n",
    "    \n",
    "###create vectorizer\n",
    "vectorizer = CountVectorizer(decode_error=\"ignore\",\n",
    "#                                tokenizer=tokenizer,\n",
    "                               stop_words=stop_words,\n",
    "                               ngram_range=ngram_range,\n",
    "                               lowercase=lowercase,\n",
    "                               binary=binary,\n",
    "                               dtype=dtype,\n",
    "                               max_features=max_features)\n",
    "\n",
    "###create pipeline\n",
    "classifier = \"MNB\"\n",
    "text_clf = Pipeline([\n",
    "                     ('vect', vectorizer),\n",
    "                     ('clf', MultinomialNB(alpha=0.1)),\n",
    "#                     ('clf', LinearSVC(C=1.0)),\n",
    "#         (\"clf\", RandomForestClassifier(n_estimators=100, max_features=\"auto\", criterion=\"entropy\", n_jobs=-1)),\n",
    "                    ])\n",
    "\n",
    "###fit training data\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "\n",
    "### predict on test data\n",
    "predicted = text_clf.predict(X_holdout)\n",
    "# np.mean(predicted == y_test)\n",
    "\n",
    "###get performance metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "accuracy = accuracy_score(y_holdout,predicted)\n",
    "#         accuracy = np.mean(y_CVtest == predicted)\n",
    "precision, recall, fscore, sup = precision_recall_fscore_support(y_holdout, predicted, average='binary', pos_label=0)\n",
    "print classifier, accuracy, precision, recall, fscore# np.mean(predicted == y_test)\n",
    "###\n",
    "###create pipeline\n",
    "classifier = \"SVC\"\n",
    "text_clf = Pipeline([\n",
    "                     ('vect', vectorizer),\n",
    "#                      ('clf', MultinomialNB(alpha=0.1)),\n",
    "                    ('clf', LinearSVC(C=1.0)),\n",
    "#         (\"clf\", RandomForestClassifier(n_estimators=100, max_features=\"auto\", criterion=\"entropy\", n_jobs=-1)),\n",
    "                    ])\n",
    "\n",
    "###fit training data\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "\n",
    "### predict on test data\n",
    "predicted = text_clf.predict(X_holdout)\n",
    "# np.mean(predicted == y_test)\n",
    "\n",
    "###get performance metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "accuracy = accuracy_score(y_holdout,predicted)\n",
    "#         accuracy = np.mean(y_CVtest == predicted)\n",
    "precision, recall, fscore, sup = precision_recall_fscore_support(y_holdout, predicted, average='binary', pos_label=0)\n",
    "print classifier, accuracy, precision, recall, fscore# np.mean(predicted == y_test)\n",
    "###\n",
    "###create pipeline\n",
    "classifier = \"RF\"\n",
    "text_clf = Pipeline([\n",
    "                     ('vect', vectorizer),\n",
    "#                      ('clf', MultinomialNB(alpha=0.1)),\n",
    "#                     ('clf', LinearSVC(C=1.0)),\n",
    "        (\"clf\", RandomForestClassifier(n_estimators=100, max_features=\"auto\", criterion=\"entropy\", n_jobs=-1)),\n",
    "                    ])\n",
    "\n",
    "###fit training data\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "\n",
    "### predict on test data\n",
    "predicted = text_clf.predict(X_holdout)\n",
    "# np.mean(predicted == y_test)\n",
    "\n",
    "###get performance metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "accuracy = accuracy_score(y_holdout,predicted)\n",
    "#         accuracy = np.mean(y_CVtest == predicted)\n",
    "precision, recall, fscore, sup = precision_recall_fscore_support(y_holdout, predicted, average='binary', pos_label=0)\n",
    "print classifier, accuracy, precision, recall, fscore# np.mean(predicted == y_test)\n",
    "###\n",
    "\n",
    "\n",
    "print \"Prediction on Holdout (20%) = DONE!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB 0.952418581024 0.93775978998 0.969029049395 0.953138028795\n",
      "SVC 0.961985663487 0.964007947772 0.959703854414 0.961851086125\n",
      "RF 0.96562623469 0.978064066852 0.952526280095 0.965126266964\n",
      "Prediction on Test (#happy, #sad) = DONE!\n"
     ]
    }
   ],
   "source": [
    "### build pipeline; fit train; predict on test\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "##vectorizer arguments blah!\n",
    "\n",
    "tokenizer=None#word_tokenize\n",
    "stop_words=nltk.corpus.stopwords.words(\"english\")#None\n",
    "ngram_range=(1, 3)\n",
    "lowercase=True\n",
    "max_features=5000\n",
    "binary=False\n",
    "dtype=np.float64\n",
    "    \n",
    "###create vectorizer\n",
    "vectorizer = CountVectorizer(decode_error=\"ignore\",\n",
    "#                                tokenizer=tokenizer,\n",
    "                               stop_words=stop_words,\n",
    "                               ngram_range=ngram_range,\n",
    "                               lowercase=lowercase,\n",
    "                               binary=binary,\n",
    "                               dtype=dtype,\n",
    "                               max_features=max_features)\n",
    "\n",
    "###create pipeline\n",
    "classifier = \"MNB\"\n",
    "text_clf = Pipeline([\n",
    "                     ('vect', vectorizer),\n",
    "                     ('clf', MultinomialNB(alpha=0.1)),\n",
    "#                     ('clf', LinearSVC(C=1.0)),\n",
    "#         (\"clf\", RandomForestClassifier(n_estimators=100, max_features=\"auto\", criterion=\"entropy\", n_jobs=-1)),\n",
    "                    ])\n",
    "\n",
    "###fit training data\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "\n",
    "### predict on test data\n",
    "predicted = text_clf.predict(X_test)\n",
    "# np.mean(predicted == y_test)\n",
    "\n",
    "###get performance metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "accuracy = accuracy_score(y_test,predicted)\n",
    "#         accuracy = np.mean(y_CVtest == predicted)\n",
    "precision, recall, fscore, sup = precision_recall_fscore_support(y_test, predicted, average='binary', pos_label=0)\n",
    "print classifier, accuracy, precision, recall, fscore# np.mean(predicted == y_test)\n",
    "###\n",
    "###create pipeline\n",
    "classifier = \"SVC\"\n",
    "text_clf = Pipeline([\n",
    "                     ('vect', vectorizer),\n",
    "#                      ('clf', MultinomialNB(alpha=0.1)),\n",
    "                    ('clf', LinearSVC(C=1.0)),\n",
    "#         (\"clf\", RandomForestClassifier(n_estimators=100, max_features=\"auto\", criterion=\"entropy\", n_jobs=-1)),\n",
    "                    ])\n",
    "\n",
    "###fit training data\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "\n",
    "### predict on test data\n",
    "predicted = text_clf.predict(X_test)\n",
    "# np.mean(predicted == y_test)\n",
    "\n",
    "###get performance metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "accuracy = accuracy_score(y_test,predicted)\n",
    "#         accuracy = np.mean(y_CVtest == predicted)\n",
    "precision, recall, fscore, sup = precision_recall_fscore_support(y_test, predicted, average='binary', pos_label=0)\n",
    "print classifier, accuracy, precision, recall, fscore# np.mean(predicted == y_test)\n",
    "###\n",
    "###create pipeline\n",
    "classifier = \"RF\"\n",
    "text_clf = Pipeline([\n",
    "                     ('vect', vectorizer),\n",
    "#                      ('clf', MultinomialNB(alpha=0.1)),\n",
    "#                     ('clf', LinearSVC(C=1.0)),\n",
    "        (\"clf\", RandomForestClassifier(n_estimators=100, max_features=\"auto\", criterion=\"entropy\", n_jobs=-1)),\n",
    "                    ])\n",
    "\n",
    "###fit training data\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "\n",
    "### predict on test data\n",
    "predicted = text_clf.predict(X_test)\n",
    "# np.mean(predicted == y_test)\n",
    "\n",
    "###get performance metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "accuracy = accuracy_score(y_test,predicted)\n",
    "#         accuracy = np.mean(y_CVtest == predicted)\n",
    "precision, recall, fscore, sup = precision_recall_fscore_support(y_test, predicted, average='binary', pos_label=0)\n",
    "print classifier, accuracy, precision, recall, fscore# np.mean(predicted == y_test)\n",
    "###\n",
    "\n",
    "\n",
    "print \"Prediction on Test (#happy, #sad) = DONE!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data = COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "###test on preprocessed data\n",
    "\n",
    "##### load training data\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# suffix = \"\"\n",
    "# suffix = \"nohashtags\"\n",
    "suffix = \"preprocessed_\"\n",
    "###read tagged sentiment Tweets\n",
    "with open('data/'+suffix+'pos_examples_PosSentiment.txt', 'r') as f:\n",
    "    pos_tweets = f.readlines()\n",
    "\n",
    "with open('data/'+suffix+'neg_examples_NegSentiment.txt', 'r') as f:\n",
    "    neg_tweets = f.readlines()\n",
    "    \n",
    "class_label = 0\n",
    "train_pos = pd.DataFrame(pos_tweets, columns = ['text'])\n",
    "X0 = train_pos.text\n",
    "y0 = np.full(shape=len(X0), fill_value=class_label)\n",
    "\n",
    "###create random 80-20 split\n",
    "rows = random.sample(X0.index, int(0.8*len(X0)))\n",
    "X0_80 = X0.ix[rows]\n",
    "X0_20 = X0.drop(rows)\n",
    "y0_80 = y0[:int(0.8*len(X0))]\n",
    "y0_20 = y0[int(0.8*len(X0)):]\n",
    "###\n",
    "\n",
    "class_label = 1\n",
    "train_neg = pd.DataFrame(neg_tweets, columns = ['text'])\n",
    "X1 = train_neg.text\n",
    "y1 = np.full(shape=len(X1), fill_value=class_label)\n",
    "\n",
    "###create random 80-20 split\n",
    "rows = random.sample(X1.index, int(0.8*len(X1)))\n",
    "X1_80 = X1.ix[rows]\n",
    "X1_20 = X1.drop(rows)\n",
    "y1_80 = y1[:int(0.8*len(X1))]\n",
    "y1_20 = y1[int(0.8*len(X1)):]\n",
    "###\n",
    "\n",
    "X_train = pd.concat((X0_80,X1_80))\n",
    "y_train = np.concatenate((y0_80, y1_80))\n",
    "\n",
    "X_holdout = pd.concat((X0_20,X1_20))\n",
    "y_holdout = np.concatenate((y0_20, y1_20))\n",
    "\n",
    "### load testing data\n",
    "\n",
    "###read tagged sentiment Tweets\n",
    "with open('data/'+suffix+'pos_examples_happy.txt', 'r') as f:\n",
    "    pos_tweets = f.readlines()\n",
    "\n",
    "with open('data/'+suffix+'neg_examples_sad.txt', 'r') as f:\n",
    "    neg_tweets = f.readlines()\n",
    "    \n",
    "import numpy as np\n",
    "class_label = 0\n",
    "train_pos = pd.DataFrame(pos_tweets, columns = ['text'])\n",
    "X0 = train_pos.text\n",
    "y0 = np.full(shape=len(X0), fill_value=class_label)\n",
    "\n",
    "class_label = 1\n",
    "train_neg = pd.DataFrame(neg_tweets, columns = ['text'])\n",
    "X1 = train_neg.text\n",
    "y1 = np.full(shape=len(X1), fill_value=class_label)\n",
    "\n",
    "X_test = pd.concat((X0,X1))\n",
    "y_test = np.concatenate((y0, y1))\n",
    "\n",
    "###\n",
    "print \"Loading data = COMPLETE!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB 0.645518602029 0.598271727131 0.885907553551 0.714217546411\n",
      "SVC 0.666065388952 0.614496696463 0.891262683202 0.727444214401\n",
      "RF 0.631313416009 0.592561687925 0.840642615558 0.695131330552\n",
      "Prediction on Test (#happy, #sad) w/0 hashtags = DONE!\n"
     ]
    }
   ],
   "source": [
    "### build pipeline; fit train; predict on test W/O Hashtags!\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "##vectorizer arguments blah!\n",
    "\n",
    "tokenizer=None#word_tokenize\n",
    "stop_words=nltk.corpus.stopwords.words(\"english\")#None\n",
    "ngram_range=(1, 3)\n",
    "lowercase=True\n",
    "max_features=5000\n",
    "binary=False\n",
    "dtype=np.float64\n",
    "    \n",
    "###create vectorizer\n",
    "vectorizer = CountVectorizer(decode_error=\"ignore\",\n",
    "#                                tokenizer=tokenizer,\n",
    "                               stop_words=stop_words,\n",
    "                               ngram_range=ngram_range,\n",
    "                               lowercase=lowercase,\n",
    "                               binary=binary,\n",
    "                               dtype=dtype,\n",
    "                               max_features=max_features)\n",
    "\n",
    "###create pipeline\n",
    "classifier = \"MNB\"\n",
    "text_clf = Pipeline([\n",
    "                     ('vect', vectorizer),\n",
    "                     ('clf', MultinomialNB(alpha=0.1)),\n",
    "#                     ('clf', LinearSVC(C=1.0)),\n",
    "#         (\"clf\", RandomForestClassifier(n_estimators=100, max_features=\"auto\", criterion=\"entropy\", n_jobs=-1)),\n",
    "                    ])\n",
    "\n",
    "###fit training data\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "\n",
    "### predict on test data\n",
    "predicted = text_clf.predict(X_test)\n",
    "# np.mean(predicted == y_test)\n",
    "\n",
    "###get performance metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "accuracy = accuracy_score(y_test,predicted)\n",
    "#         accuracy = np.mean(y_CVtest == predicted)\n",
    "precision, recall, fscore, sup = precision_recall_fscore_support(y_test, predicted, average='binary', pos_label=0)\n",
    "print classifier, accuracy, precision, recall, fscore# np.mean(predicted == y_test)\n",
    "###\n",
    "###create pipeline\n",
    "classifier = \"SVC\"\n",
    "text_clf = Pipeline([\n",
    "                     ('vect', vectorizer),\n",
    "#                      ('clf', MultinomialNB(alpha=0.1)),\n",
    "                    ('clf', LinearSVC(C=1.0)),\n",
    "#         (\"clf\", RandomForestClassifier(n_estimators=100, max_features=\"auto\", criterion=\"entropy\", n_jobs=-1)),\n",
    "                    ])\n",
    "\n",
    "###fit training data\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "\n",
    "### predict on test data\n",
    "predicted = text_clf.predict(X_test)\n",
    "# np.mean(predicted == y_test)\n",
    "\n",
    "###get performance metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "accuracy = accuracy_score(y_test,predicted)\n",
    "#         accuracy = np.mean(y_CVtest == predicted)\n",
    "precision, recall, fscore, sup = precision_recall_fscore_support(y_test, predicted, average='binary', pos_label=0)\n",
    "print classifier, accuracy, precision, recall, fscore# np.mean(predicted == y_test)\n",
    "###\n",
    "###create pipeline\n",
    "classifier = \"RF\"\n",
    "text_clf = Pipeline([\n",
    "                     ('vect', vectorizer),\n",
    "#                      ('clf', MultinomialNB(alpha=0.1)),\n",
    "#                     ('clf', LinearSVC(C=1.0)),\n",
    "        (\"clf\", RandomForestClassifier(n_estimators=100, max_features=\"auto\", criterion=\"entropy\", n_jobs=-1)),\n",
    "                    ])\n",
    "\n",
    "###fit training data\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "\n",
    "### predict on test data\n",
    "predicted = text_clf.predict(X_test)\n",
    "# np.mean(predicted == y_test)\n",
    "\n",
    "###get performance metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "accuracy = accuracy_score(y_test,predicted)\n",
    "#         accuracy = np.mean(y_CVtest == predicted)\n",
    "precision, recall, fscore, sup = precision_recall_fscore_support(y_test, predicted, average='binary', pos_label=0)\n",
    "print classifier, accuracy, precision, recall, fscore# np.mean(predicted == y_test)\n",
    "###\n",
    "\n",
    "\n",
    "print \"Prediction on Test (#happy, #sad) w/0 hashtags = DONE!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###test on preprocessed data\n",
    "\n",
    "##### load training data\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# suffix = \"\"\n",
    "# suffix = \"nohashtags\"\n",
    "suffix = \"preprocessed_\"\n",
    "###read tagged sentiment Tweets\n",
    "with open('data/'+suffix+'pos_examples_PosSentiment.txt', 'r') as f:\n",
    "    pos_tweets = f.readlines()\n",
    "\n",
    "with open('data/'+suffix+'neg_examples_NegSentiment.txt', 'r') as f:\n",
    "    neg_tweets = f.readlines()\n",
    "    \n",
    "class_label = 0\n",
    "train_pos = pd.DataFrame(pos_tweets, columns = ['text'])\n",
    "X0 = train_pos.text\n",
    "y0 = np.full(shape=len(X0), fill_value=class_label)\n",
    "\n",
    "###create random 80-20 split\n",
    "rows = random.sample(X0.index, int(0.8*len(X0)))\n",
    "X0_80 = X0.ix[rows]\n",
    "X0_20 = X0.drop(rows)\n",
    "y0_80 = y0[:int(0.8*len(X0))]\n",
    "y0_20 = y0[int(0.8*len(X0)):]\n",
    "###\n",
    "\n",
    "class_label = 1\n",
    "train_neg = pd.DataFrame(neg_tweets, columns = ['text'])\n",
    "X1 = train_neg.text\n",
    "y1 = np.full(shape=len(X1), fill_value=class_label)\n",
    "\n",
    "###create random 80-20 split\n",
    "rows = random.sample(X1.index, int(0.8*len(X1)))\n",
    "X1_80 = X1.ix[rows]\n",
    "X1_20 = X1.drop(rows)\n",
    "y1_80 = y1[:int(0.8*len(X1))]\n",
    "y1_20 = y1[int(0.8*len(X1)):]\n",
    "###\n",
    "\n",
    "X_train = pd.concat((X0_80,X1_80))\n",
    "y_train = np.concatenate((y0_80, y1_80))\n",
    "\n",
    "X_holdout = pd.concat((X0_20,X1_20))\n",
    "y_holdout = np.concatenate((y0_20, y1_20))\n",
    "\n",
    "### load testing data\n",
    "\n",
    "###read tagged sentiment Tweets\n",
    "with open('data/'+suffix+'pos_examples_happy.txt', 'r') as f:\n",
    "    pos_tweets = f.readlines()\n",
    "\n",
    "with open('data/'+suffix+'neg_examples_sad.txt', 'r') as f:\n",
    "    neg_tweets = f.readlines()\n",
    "    \n",
    "import numpy as np\n",
    "class_label = 0\n",
    "train_pos = pd.DataFrame(pos_tweets, columns = ['text'])\n",
    "X0 = train_pos.text\n",
    "y0 = np.full(shape=len(X0), fill_value=class_label)\n",
    "\n",
    "class_label = 1\n",
    "train_neg = pd.DataFrame(neg_tweets, columns = ['text'])\n",
    "X1 = train_neg.text\n",
    "y1 = np.full(shape=len(X1), fill_value=class_label)\n",
    "\n",
    "X_test = pd.concat((X0,X1))\n",
    "y_test = np.concatenate((y0, y1))\n",
    "\n",
    "###\n",
    "print \"Loading data = COMPLETE!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1\n",
      "MNB 0.72890625 0.722150219095 0.737117298727 0.72955700346\n",
      "Fold  2\n",
      "MNB 0.724 0.738138686131 0.701734820322 0.719476559522\n",
      "Fold  3\n",
      "MNB 0.73446875 0.727764792717 0.744010564045 0.735798016231\n",
      "Fold  4\n",
      "MNB 0.72784375 0.743854344683 0.701655403311 0.72213891459\n",
      "Fold  5\n",
      "MNB 0.735 0.730964153276 0.741318791526 0.736105060061\n"
     ]
    }
   ],
   "source": [
    "### build pipeline; fit on BEST CV MODEL train; predict on test!\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "##vectorizer arguments blah!\n",
    "\n",
    "tokenizer=None#word_tokenize\n",
    "stop_words=nltk.corpus.stopwords.words(\"english\")#None\n",
    "ngram_range=(1, 3)\n",
    "lowercase=True\n",
    "max_features=5000\n",
    "binary=False\n",
    "dtype=np.float64\n",
    "    \n",
    "###create vectorizer\n",
    "vectorizer = CountVectorizer(decode_error=\"ignore\",\n",
    "#                                tokenizer=tokenizer,\n",
    "                               stop_words=stop_words,\n",
    "                               ngram_range=ngram_range,\n",
    "                               lowercase=lowercase,\n",
    "                               binary=binary,\n",
    "                               dtype=dtype,\n",
    "                               max_features=max_features)\n",
    "\n",
    "###5-fold cross validation\n",
    "n_folds = 5\n",
    "kf = KFold(len(X_train), n_folds=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "###create pipeline\n",
    "classifier = \"MNB\"\n",
    "text_clf = Pipeline([\n",
    "                     ('vect', vectorizer),\n",
    "                     ('clf', MultinomialNB(alpha=0.1)),\n",
    "#                     ('clf', LinearSVC(C=1.0)),\n",
    "#         (\"clf\", RandomForestClassifier(n_estimators=100, max_features=\"auto\", criterion=\"entropy\", n_jobs=-1)),\n",
    "                    ])\n",
    "\n",
    "\n",
    "count = 0\n",
    "max_fscore = 0.0\n",
    "best_X_train = X_train\n",
    "best_y_train = y_train\n",
    "best_model = 0\n",
    "\n",
    "for train, test in kf:\n",
    "    count = count + 1\n",
    "    print \"Fold \", count\n",
    "    X_CVtrain, y_CVtrain, X_CVtest, y_CVtest = X_train.values[train], y_train[train], X_train.values[test], y_train[test]\n",
    "    ###fit training data\n",
    "    text_clf = text_clf.fit(X_CVtrain, y_CVtrain)\n",
    "    ### predict on test data\n",
    "    predicted = text_clf.predict(X_CVtest)\n",
    "\n",
    "    ###get performance metrics\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "    accuracy = accuracy_score(y_CVtest,predicted)\n",
    "    precision, recall, fscore, sup = precision_recall_fscore_support(y_CVtest, predicted, average='binary', pos_label=0)\n",
    "    print classifier, accuracy, precision, recall, fscore\n",
    "    \n",
    "    ###update best-model\n",
    "    if max_fscore < fscore: \n",
    "        max_fscore = fscore\n",
    "        best_X_train = X_CVtrain\n",
    "        best_y_train = y_CVtrain\n",
    "        best_X_test = X_CVtest\n",
    "        best_y_test = y_CVtest\n",
    "        best_model = count - 1\n",
    "        \n",
    "###fit the best NB model\n",
    "NB_best = text_clf.fit(best_X_train, best_y_train)\n",
    "###\n",
    "\n",
    "###create pipeline\n",
    "classifier = \"LinearSVC\"\n",
    "text_clf = Pipeline([\n",
    "                     ('vect', vectorizer),\n",
    "#                      ('clf', MultinomialNB(alpha=0.1)),\n",
    "                    ('clf', LinearSVC(C=1.0)),\n",
    "#         (\"clf\", RandomForestClassifier(n_estimators=100, max_features=\"auto\", criterion=\"entropy\", n_jobs=-1)),\n",
    "                    ])\n",
    "\n",
    "\n",
    "count = 0\n",
    "max_fscore = 0.0\n",
    "best_X_train = X_train\n",
    "best_y_train = y_train\n",
    "best_model = 0\n",
    "\n",
    "for train, test in kf:\n",
    "    count = count + 1\n",
    "    print \"Fold \", count\n",
    "    X_CVtrain, y_CVtrain, X_CVtest, y_CVtest = X_train.values[train], y_train[train], X_train.values[test], y_train[test]\n",
    "    ###fit training data\n",
    "    text_clf = text_clf.fit(X_CVtrain, y_CVtrain)\n",
    "    ### predict on test data\n",
    "    predicted = text_clf.predict(X_CVtest)\n",
    "\n",
    "    ###get performance metrics\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "    accuracy = accuracy_score(y_CVtest,predicted)\n",
    "    precision, recall, fscore, sup = precision_recall_fscore_support(y_CVtest, predicted, average='binary', pos_label=0)\n",
    "    print classifier, accuracy, precision, recall, fscore\n",
    "    \n",
    "    ###update best-model\n",
    "    if max_fscore < fscore: \n",
    "        max_fscore = fscore\n",
    "        best_X_train = X_CVtrain\n",
    "        best_y_train = y_CVtrain\n",
    "        best_X_test = X_CVtest\n",
    "        best_y_test = y_CVtest\n",
    "        best_model = count - 1\n",
    "        \n",
    "###fit the best SVC model\n",
    "SVC_best = text_clf.fit(best_X_train, best_y_train)\n",
    "\n",
    "###\n",
    "\n",
    "###create pipeline\n",
    "classifier = \"RandomForest\"\n",
    "text_clf = Pipeline([\n",
    "                     ('vect', vectorizer),\n",
    "#                      ('clf', MultinomialNB(alpha=0.1)),\n",
    "#                     ('clf', LinearSVC(C=1.0)),\n",
    "        (\"clf\", RandomForestClassifier(n_estimators=100, max_features=\"auto\", criterion=\"entropy\", n_jobs=-1)),\n",
    "                    ])\n",
    "\n",
    "\n",
    "count = 0\n",
    "max_fscore = 0.0\n",
    "best_X_train = X_train\n",
    "best_y_train = y_train\n",
    "best_model = 0\n",
    "\n",
    "for train, test in kf:\n",
    "    count = count + 1\n",
    "    print \"Fold \", count\n",
    "    X_CVtrain, y_CVtrain, X_CVtest, y_CVtest = X_train.values[train], y_train[train], X_train.values[test], y_train[test]\n",
    "    ###fit training data\n",
    "    text_clf = text_clf.fit(X_CVtrain, y_CVtrain)\n",
    "    ### predict on test data\n",
    "    predicted = text_clf.predict(X_CVtest)\n",
    "\n",
    "    ###get performance metrics\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "    accuracy = accuracy_score(y_CVtest,predicted)\n",
    "    precision, recall, fscore, sup = precision_recall_fscore_support(y_CVtest, predicted, average='binary', pos_label=0)\n",
    "    print classifier, accuracy, precision, recall, fscore\n",
    "    \n",
    "    ###update best-model\n",
    "    if max_fscore < fscore: \n",
    "        max_fscore = fscore\n",
    "        best_X_train = X_CVtrain\n",
    "        best_y_train = y_CVtrain\n",
    "        best_X_test = X_CVtest\n",
    "        best_y_test = y_CVtest\n",
    "        best_model = count - 1\n",
    "        \n",
    "###fit the best RF model\n",
    "RF_best = text_clf.fit(best_X_train, best_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate best model\n",
      "MNB 0.735 0.730964153276 0.741318791526 0.736105060061\n"
     ]
    }
   ],
   "source": [
    "print \"validate best model\"\n",
    "text_clf = text_clf.fit(best_X_train, best_y_train)\n",
    "### predict on test data\n",
    "predicted = text_clf.predict(best_X_test)\n",
    "\n",
    "###get performance metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "accuracy = accuracy_score(best_y_test,predicted)\n",
    "precision, recall, fscore, sup = precision_recall_fscore_support(best_y_test, predicted, average='binary', pos_label=0)\n",
    "print classifier, accuracy, precision, recall, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
